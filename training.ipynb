{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14444996,"sourceType":"datasetVersion","datasetId":9214451}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":46.911358,"end_time":"2026-01-10T12:29:16.996031","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-01-10T12:28:30.084673","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"682a3961","cell_type":"code","source":"import os\n\n# Define paths\nbase_path = '/kaggle/working'\nproject_name = 'Double-Pendulum-Simulation'\nproject_root = os.path.join(base_path, project_name)\n\n# Clone or pull repository\nif not os.path.exists(project_root):\n    os.chdir(base_path)\n    !git clone https://github.com/nthday-jpg/Double-Pendulum-Simulation.git\n    print(\"Repository cloned successfully!\")\nelse:\n    os.chdir(project_root)\n    !git pull\n    print(\"Repository updated successfully!\")\n%cd {base_path}","metadata":{"papermill":{"duration":0.75407,"end_time":"2026-01-10T12:28:33.434095","exception":false,"start_time":"2026-01-10T12:28:32.680025","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"2acfbaf6","cell_type":"code","source":"# Generate training data\n# Generate 10 trajectories with 5000 points each\n!python {project_root}/generate_data.py \\\n    --output_dir {project_root}/data/raw \\\n    --num_trajectories 20 \\\n    --num_points 3000 \\\n    --t_start 0.0 \\\n    --t_end 2.0 \\\n    --check_energy","metadata":{"execution":{"iopub.status.busy":"2026-01-12T15:24:49.980107Z","iopub.execute_input":"2026-01-12T15:24:49.980413Z","iopub.status.idle":"2026-01-12T15:24:52.634604Z","shell.execute_reply.started":"2026-01-12T15:24:49.980384Z","shell.execute_reply":"2026-01-12T15:24:52.633917Z"},"papermill":{"duration":4.775486,"end_time":"2026-01-10T12:28:38.211847","exception":false,"start_time":"2026-01-10T12:28:33.436361","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Generating 20 trajectories...\nDeriving equations symbolically (this may take a moment)...\nSymbolic derivation complete!\n  Trajectory 000 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_000.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_000.json\n  Trajectory 001 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_001.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_001.json\n  Trajectory 002 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_002.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_002.json\n  Trajectory 003 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_003.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_003.json\n  Trajectory 004 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_004.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_004.json\n  Trajectory 005 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_005.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_005.json\n  Trajectory 006 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_006.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_006.json\n  Trajectory 007 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_007.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_007.json\n  Trajectory 008 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_008.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_008.json\n  Trajectory 009 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_009.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_009.json\n  Trajectory 010 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_010.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_010.json\n  Trajectory 011 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_011.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_011.json\n  Trajectory 012 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_012.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_012.json\n  Trajectory 013 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_013.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_013.json\n  Trajectory 014 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_014.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_014.json\n  Trajectory 015 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_015.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_015.json\n  Trajectory 016 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_016.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_016.json\n  Trajectory 017 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_017.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_017.json\n  Trajectory 018 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_018.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_018.json\n  Trajectory 019 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_019.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_019.json\n\nDataset complete! Saved to /kaggle/working/Double-Pendulum-Simulation/data/raw\n","output_type":"stream"}],"execution_count":67},{"id":"5f7722e2","cell_type":"code","source":"# Remove old dataset\n!rm -rf /kaggle/working/Double-Pendulum-Simulation/data/raw/","metadata":{"execution":{"iopub.execute_input":"2026-01-10T12:28:38.217560Z","iopub.status.busy":"2026-01-10T12:28:38.217278Z","iopub.status.idle":"2026-01-10T12:28:38.339965Z","shell.execute_reply":"2026-01-10T12:28:38.338868Z"},"papermill":{"duration":0.12773,"end_time":"2026-01-10T12:28:38.341818","exception":false,"start_time":"2026-01-10T12:28:38.214088","status":"completed"},"tags":[]},"outputs":[],"execution_count":3},{"id":"368af10d","cell_type":"code","source":"# Training Configuration\n# Experiment\nseed = 42\nrun_name = None  # Auto-generated if None\ncheckpoint_path = None#\"runs/run_20260112_135314/checkpoints/best_model.pth\"\n\n# Data\ndata_dir = f\"{project_root}/data/raw\"\nval_split = 0.2\ntest_split = 0.1\nnormalize_time = True\nnormalize_state = False\n\n# Model Architecture\nmodel = \"pinn\"  # mlp | neural_ode | hnn | pinn\nhidden_dims = \"64 64 64 64\"  # Space-separated\nactivation = \"tanh\"  # tanh | relu | gelu | silu | softplus\nuse_batch_norm = False\ndropout_rate = 0.0\nfinal_activation = None  # None | tanh | sigmoid\ninput_dim = 5\noutput_dim = 2\n\n# Training\nlr = 0.0005\nbatch_size = 128\nbatch_size_collocation = 1024\nepochs = 500\noptimizer = \"adam\"  # adam | adamw | sgd\nweight_decay = 0.0\ngrad_clip = None\nscheduler = None  # None | cosine | step\n\n# PyTorch Optimizations\nuse_compile = False\ncompile_mode = \"default\"  # default | reduce-overhead | max-autotune\nmixed_precision = False\ngradient_accumulation_steps = 1\n\n# Regularization\nl1_lambda = 0.0\nl2_lambda = 0.001\n\n# Physics / PINN\nuse_physics = True\nn_collocation = 5000\ndata_fraction = 0.5\ndata_loss_ratio = 0.99\nresidual_type = \"lagrangian\"  # eom | lagrangian | hamiltonian\n\n# Time Domain\nt_min = 0.0\nt_max = 3.0\ncollocation_sampling = \"uniform\"  # uniform | random | latin_hypercube\n\n# Rollout Evaluation\nrollout_T = 5.0\nrollout_dt = 0.01\n\n# Logging\nlog_interval = 10\nprint_interval = 50\nsave_checkpoints = True\ncheckpoint_interval = 50\ntest_interval = 100\n\n# Early Stopping\nearly_stopping_patience = 100  # None to disable\n\n# Physical Parameters\nm1 = 1.0\nm2 = 1.0\nl1 = 1.0\nl2 = 1.0\ng = 9.81\n\n# Build command arguments\nargs_list = [\n    f\"--seed {seed}\",\n    f\"--hidden_dims {hidden_dims}\",\n    f\"--input_dim {input_dim}\",\n    f\"--output_dim {output_dim}\",\n    f\"--residual_type {residual_type}\",\n    f\"--t_max {t_max}\",\n    f\"--t_min {t_min}\",\n    f\"--epochs {epochs}\",\n    f\"--lr {lr}\",\n    f\"--batch_size {batch_size}\",\n    f\"--batch_size_collocation {batch_size_collocation}\",\n    f\"--data_dir {data_dir}\",\n    f\"--val_split {val_split}\",\n    f\"--test_split {test_split}\",\n    f\"--model {model}\",\n    f\"--activation {activation}\",\n    f\"--dropout_rate {dropout_rate}\",\n    f\"--optimizer {optimizer}\",\n    f\"--weight_decay {weight_decay}\",\n    f\"--compile_mode {compile_mode}\",\n    f\"--gradient_accumulation_steps {gradient_accumulation_steps}\",\n    f\"--l1_lambda {l1_lambda}\",\n    f\"--l2_lambda {l2_lambda}\",\n    f\"--n_collocation {n_collocation}\",\n    f\"--data_fraction {data_fraction}\",\n    f\"--data_loss_ratio {data_loss_ratio}\",\n    f\"--collocation_sampling {collocation_sampling}\",\n    f\"--rollout_T {rollout_T}\",\n    f\"--rollout_dt {rollout_dt}\",\n    f\"--log_interval {log_interval}\",\n    f\"--print_interval {print_interval}\",\n    f\"--test_interval {test_interval}\",\n    f\"--checkpoint_interval {checkpoint_interval}\",\n    f\"--m1 {m1}\",\n    f\"--m2 {m2}\",\n    f\"--l1 {l1}\",\n    f\"--l2 {l2}\",\n    f\"--g {g}\"\n]\n\n# Add optional flags\nif run_name:\n    args_list.append(f\"--run_name {run_name}\")\nif checkpoint_path:\n    args_list.append(f\"--checkpoint_path {checkpoint_path}\")\nif use_compile:\n    args_list.append(\"--use_compile\")\nif normalize_time:\n    args_list.append(\"--normalize_time\")\nif normalize_state:\n    args_list.append(\"--normalize_state\")\nif use_batch_norm:\n    args_list.append(\"--use_batch_norm\")\nif final_activation:\n    args_list.append(f\"--final_activation {final_activation}\")\nif grad_clip:\n    args_list.append(f\"--grad_clip {grad_clip}\")\nif scheduler:\n    args_list.append(f\"--scheduler {scheduler}\")\nif mixed_precision:\n    args_list.append(\"--mixed_precision\")\nif save_checkpoints:\n    args_list.append(\"--save_checkpoints\")\nif early_stopping_patience:\n    args_list.append(f\"--early_stopping_patience {early_stopping_patience}\")\nif use_physics:\n    args_list.append(\"--use_physics\")\n\nargs = \" \".join(args_list)\n\nprint(f\"Training Configuration:\")\nprint(f\"  Seed: {seed}\")\nprint(f\"  Model: {model}, Hidden: {hidden_dims}, Activation: {activation}\")\nprint(f\"  Optimizer: {optimizer}, LR: {lr}, Weight Decay: {weight_decay}\")\nprint(f\"  Epochs: {epochs}, Early Stop: {early_stopping_patience if early_stopping_patience else 'disabled'}\")\nprint(f\"  Batch sizes: data={batch_size}, colloc={batch_size_collocation}\")\nprint(f\"  Data loss ratio = {data_loss_ratio}, type={residual_type}\")\nprint(f\"  Checkpoint: {checkpoint_path if checkpoint_path else 'None (training from scratch)'}\")\nprint(f\"\\nCommand arguments generated with {len(args_list)} parameters\")\n","metadata":{"execution":{"iopub.status.busy":"2026-01-12T15:25:16.799486Z","iopub.execute_input":"2026-01-12T15:25:16.800031Z","iopub.status.idle":"2026-01-12T15:25:16.812723Z","shell.execute_reply.started":"2026-01-12T15:25:16.800002Z","shell.execute_reply":"2026-01-12T15:25:16.811956Z"},"papermill":{"duration":0.018811,"end_time":"2026-01-10T12:28:38.363040","exception":false,"start_time":"2026-01-10T12:28:38.344229","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Training Configuration:\n  Seed: 42\n  Model: pinn, Hidden: 64 64 64 64, Activation: tanh\n  Optimizer: adam, LR: 0.0005, Weight Decay: 0.0\n  Epochs: 500, Early Stop: 100\n  Batch sizes: data=128, colloc=1024\n  Data loss ratio = 0.99, type=lagrangian\n  Checkpoint: None (training from scratch)\n\nCommand arguments generated with 42 parameters\n","output_type":"stream"}],"execution_count":69},{"id":"0632a621","cell_type":"code","source":"# Launch distributed training with Accelerate\n!accelerate launch --num_processes=2 {project_root}/train.py {args}","metadata":{"execution":{"iopub.status.busy":"2026-01-12T15:25:21.986956Z","iopub.execute_input":"2026-01-12T15:25:21.987510Z","iopub.status.idle":"2026-01-12T15:56:40.547912Z","shell.execute_reply.started":"2026-01-12T15:25:21.987481Z","shell.execute_reply":"2026-01-12T15:56:40.547198Z"},"papermill":{"duration":38.285609,"end_time":"2026-01-10T12:29:16.650642","exception":false,"start_time":"2026-01-10T12:28:38.365033","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\nThe following values were not passed to `accelerate launch` and had defaults used instead:\n\t\tMore than one GPU was found, enabling multi-GPU training.\n\t\tIf this was unintended please pass in `--num_processes=1`.\n\t`--num_machines` was set to a value of `1`\n\t`--mixed_precision` was set to a value of `'no'`\n\t`--dynamo_backend` was set to a value of `'no'`\nTo avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768231532.552036  221803 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768231532.552047  221804 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768231532.558683  221804 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1768231532.558684  221803 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768231532.576115  221804 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768231532.576109  221803 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768231532.576142  221804 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768231532.576147  221803 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768231532.576151  221804 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768231532.576154  221803 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768231532.576159  221804 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768231532.576165  221803 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nðŸŒ± Random seed set to: 42\nðŸŒ± Random seed set to: 42\nðŸ“Š Loaded 20 trajectories with shared parameters, 60000 total data points\nðŸ“Š Loaded 20 trajectories with shared parameters, 60000 total data points\nDataLoaders: data_bs=128, colloc_bs=1024, workers=4\nDataset splits: train=43200, val=10800, test=6000 (late time)\nTemporal split: train/val use first 90% of time, test uses last 10%\n[W112 15:25:36.273067198 Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n[W112 15:25:36.273094324 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\nDataLoaders: data_bs=128, colloc_bs=1024, workers=4\nDataset splits: train=43200, val=10800, test=6000 (late time)\nTemporal split: train/val use first 90% of time, test uses last 10%\n[W112 15:25:36.283853251 Utils.hpp:166] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n[W112 15:25:36.283874064 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\nNCCL version 2.27.3+cuda12.9\n[rank0]:[W112 15:25:37.885317837 Utils.hpp:112] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\n[rank1]:[W112 15:25:37.887528454 Utils.hpp:112] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())\nðŸ“Š Logging to: runs/run_20260112_152535\n============================================================\nStarting training for 500 epochs...\nDevice: cuda:0\nData batch: 128, Collocation batch: 1024\n============================================================\n\n================================================================================\nEpoch           1/500\n================================================================================\nDataset         Total Loss      Physics Loss    Data Loss      \n--------------------------------------------------------------------------------\nTrain           0.068346        0.781412        0.061143       \nValidation      0.514008        0.911209        0.509996       \n================================================================================\n\n  â†’ New best model saved (val_loss: 0.514008)\n  â†’ New best model saved (val_loss: 0.421336)\n  â†’ New best model saved (val_loss: 0.395632)\n  â†’ New best model saved (val_loss: 0.384820)\n  â†’ New best model saved (val_loss: 0.365204)\n  â†’ New best model saved (val_loss: 0.360618)\n  â†’ New best model saved (val_loss: 0.341445)\n  â†’ New best model saved (val_loss: 0.330464)\n  â†’ New best model saved (val_loss: 0.320751)\n  â†’ New best model saved (val_loss: 0.315647)\n  â†’ New best model saved (val_loss: 0.307987)\n  â†’ New best model saved (val_loss: 0.303997)\n  â†’ New best model saved (val_loss: 0.300176)\n  â†’ New best model saved (val_loss: 0.296508)\n  â†’ New best model saved (val_loss: 0.286729)\n  â†’ New best model saved (val_loss: 0.284119)\n  â†’ New best model saved (val_loss: 0.282334)\n  â†’ New best model saved (val_loss: 0.280088)\n  â†’ New best model saved (val_loss: 0.279114)\n  â†’ New best model saved (val_loss: 0.276820)\n  â†’ New best model saved (val_loss: 0.273840)\n  â†’ New best model saved (val_loss: 0.271697)\n  â†’ New best model saved (val_loss: 0.265961)\n  â†’ New best model saved (val_loss: 0.264147)\n  â†’ New best model saved (val_loss: 0.263411)\n  â†’ New best model saved (val_loss: 0.262040)\n  â†’ New best model saved (val_loss: 0.260290)\n  â†’ New best model saved (val_loss: 0.259664)\n  â†’ New best model saved (val_loss: 0.258815)\n  â†’ New best model saved (val_loss: 0.257343)\n  â†’ New best model saved (val_loss: 0.255228)\n  â†’ New best model saved (val_loss: 0.252526)\n  â†’ New best model saved (val_loss: 0.250434)\n  â†’ New best model saved (val_loss: 0.247290)\n  â†’ New best model saved (val_loss: 0.244500)\n  â†’ New best model saved (val_loss: 0.243133)\n  â†’ New best model saved (val_loss: 0.240066)\n  â†’ New best model saved (val_loss: 0.239236)\n\n================================================================================\nEpoch           50/500\n================================================================================\nDataset         Total Loss      Physics Loss    Data Loss      \n--------------------------------------------------------------------------------\nTrain           0.040950        1.600045        0.025201       \nValidation      0.239983        1.848565        0.223734       \n================================================================================\n\n  â†’ New best model saved (val_loss: 0.239075)\n  â†’ New best model saved (val_loss: 0.238602)\n  â†’ New best model saved (val_loss: 0.238304)\n  â†’ New best model saved (val_loss: 0.237006)\n  â†’ New best model saved (val_loss: 0.236700)\n  â†’ New best model saved (val_loss: 0.236188)\n  â†’ New best model saved (val_loss: 0.236034)\n  â†’ New best model saved (val_loss: 0.235876)\n  â†’ New best model saved (val_loss: 0.235771)\n  â†’ New best model saved (val_loss: 0.234651)\n  â†’ New best model saved (val_loss: 0.234142)\n\n================================================================================\nEpoch           100/500\n================================================================================\nDataset         Total Loss      Physics Loss    Data Loss      \n--------------------------------------------------------------------------------\nTrain           0.037966        1.379616        0.024414       \nValidation      0.237393        1.991590        0.219674       \n================================================================================\n\n\n================================================================================\nEvaluating on Test Set (Temporal Extrapolation)\n================================================================================\nLoading best model from: runs/run_20260112_152535/checkpoints/best_model.pth\nEvaluating test set...\n\nTest Set Results:\n--------------------------------------------------------------------------------\nMetric                         Value          \n--------------------------------------------------------------------------------\nTotal Loss                     2.127770       \nPhysics Loss                   0.769730       \nData Loss                      2.141488       \n================================================================================\n\nTest results saved to: runs/run_20260112_152535/test_results.txt\n  â†’ New best model saved (val_loss: 0.234021)\n  â†’ New best model saved (val_loss: 0.233929)\n\n================================================================================\nEpoch           150/500\n================================================================================\nDataset         Total Loss      Physics Loss    Data Loss      \n--------------------------------------------------------------------------------\nTrain           0.037967        1.392385        0.024286       \nValidation      0.234385        1.956863        0.216986       \n================================================================================\n\n  â†’ New best model saved (val_loss: 0.233800)\n\n================================================================================\nEpoch           200/500\n================================================================================\nDataset         Total Loss      Physics Loss    Data Loss      \n--------------------------------------------------------------------------------\nTrain           0.037488        1.339005        0.024342       \nValidation      0.234416        1.827448        0.218325       \n================================================================================\n\n\n================================================================================\nEvaluating on Test Set (Temporal Extrapolation)\n================================================================================\nLoading best model from: runs/run_20260112_152535/checkpoints/best_model.pth\nEvaluating test set...\n\nTest Set Results:\n--------------------------------------------------------------------------------\nMetric                         Value          \n--------------------------------------------------------------------------------\nTotal Loss                     2.152311       \nPhysics Loss                   0.567544       \nData Loss                      2.168318       \n================================================================================\n\nTest results saved to: runs/run_20260112_152535/test_results.txt\n  â†’ New best model saved (val_loss: 0.233779)\n^C\nW0112 15:56:39.843000 221784 torch/distributed/elastic/agent/server/api.py:723] Received 2 death signal, shutting down workers\nW0112 15:56:39.845000 221784 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 221803 closing signal SIGINT\nW0112 15:56:39.846000 221784 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 221804 closing signal SIGINT\nTraining complete. Logs saved to runs/run_20260112_152535\nBest model saved at: runs/run_20260112_152535/checkpoints/best_model.pth\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/kaggle/working/Double-Pendulum-Simulation/train.py\", line 317, in <module>\n[rank1]:     main()\n[rank1]:   File \"/kaggle/working/Double-Pendulum-Simulation/train.py\", line 313, in main\n[rank1]:     trainer.train()\n[rank1]:   File \"/kaggle/working/Double-Pendulum-Simulation/training/trainer.py\", line 113, in train\n[rank1]:     total_train_loss, total_physics_loss, total_data_loss, total_samples = self._train_step(\n[rank1]:                                                                            ^^^^^^^^^^^^^^^^^\n[rank1]:   File \"/kaggle/working/Double-Pendulum-Simulation/training/trainer.py\", line 237, in _train_step\n[rank1]:     self.accelerator.backward(loss)\n[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 2740, in backward\n[rank1]:     loss.backward(**kwargs)\n[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\", line 647, in backward\n[rank1]:     torch.autograd.backward(\n[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\", line 354, in backward\n[rank1]:     _engine_run_backward(\n[rank1]:   File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\", line 829, in _engine_run_backward\n[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]: KeyboardInterrupt\n","output_type":"stream"}],"execution_count":70},{"id":"8a49c89a-1606-4be8-9753-4f316dce322f","cell_type":"code","source":"t_start = 0\nt_end = 1\nnum_points = 1000\ntheta1 = 0.5\ntheta2 = 0.2\nomega1 = 0\nomega2 = 0\ncheckpoint = f\"{base_path}/runs/run_20260112_152535/checkpoints/best_model.pth\"\noutput_dir = f\"{base_path}/runs/test_inference\"\n\narg_list = [\n    f\"{checkpoint}\",\n    f\"--output-dir {output_dir}\",\n    f\"--t-start {t_start}\",\n    f\"--t-end {t_end}\",\n    f\"--omega1 {omega1}\",\n    f\"--omega2 {omega2}\",\n    f\"--theta1 {theta1}\",\n    f\"--theta1 {theta2}\",\n]\nargs = \" \".join(arg_list)\n!python {project_root}/inference.py {args}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T15:56:48.436057Z","iopub.execute_input":"2026-01-12T15:56:48.436625Z","iopub.status.idle":"2026-01-12T15:56:53.797922Z","shell.execute_reply.started":"2026-01-12T15:56:48.436590Z","shell.execute_reply":"2026-01-12T15:56:53.797240Z"}},"outputs":[{"name":"stdout","text":"Loading model from: /kaggle/working/runs/run_20260112_152535/checkpoints/best_model.pth\nNormalization settings:\n  Time: True | Range: [0.000, 3.000]\n  Angles: True | Range: [-3.142, 3.142]\n\nInitial state: theta1=0.473, theta2=-0.635, omega1=-0.425, omega2=-0.124\nTime span: (0.0, 1.0), Points: 1000\n\nSimulating with PINN...\nSaved trajectory: /kaggle/working/runs/test_inference/trajectory_pinn.npz\nSaved parameters: /kaggle/working/runs/test_inference/trajectory_pinn_parameters.json\n\nComputing ground truth...\nDeriving equations symbolically (this may take a moment)...\nSymbolic derivation complete!\nSaved trajectory: /kaggle/working/runs/test_inference/trajectory_true.npz\nSaved parameters: /kaggle/working/runs/test_inference/trajectory_true_parameters.json\n\nGenerating comparison plots...\nComparison plot saved to: /kaggle/working/runs/test_inference/comparison_plot.png\n\nComputing metrics...\nSaved metrics: /kaggle/working/runs/test_inference/metrics.json\n\n============================================================\nINFERENCE RESULTS\n============================================================\nPosition RMSE:  0.377500\nPosition MAE:   0.310590\nVelocity MSE:   5.216902\n\nEnergy drift (true): 0.000%\nEnergy drift (pred): 6.525%\n============================================================\n\nAll results saved to: /kaggle/working/runs/test_inference\n","output_type":"stream"}],"execution_count":71},{"id":"64aae487","cell_type":"code","source":"!zip -r /kaggle/working/runs.zip runs","metadata":{"execution":{"iopub.status.busy":"2026-01-12T15:56:59.785795Z","iopub.execute_input":"2026-01-12T15:56:59.786317Z","iopub.status.idle":"2026-01-12T15:56:59.929153Z","shell.execute_reply.started":"2026-01-12T15:56:59.786282Z","shell.execute_reply":"2026-01-12T15:56:59.928478Z"},"papermill":{"duration":0.123159,"end_time":"2026-01-10T12:29:16.776566","exception":false,"start_time":"2026-01-10T12:29:16.653407","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"updating: runs/ (stored 0%)\nupdating: runs/test_inference/ (stored 0%)\nupdating: runs/test_inference/trajectory_true.npz (deflated 11%)\nupdating: runs/test_inference/trajectory_pinn.npz (deflated 24%)\nupdating: runs/test_inference/trajectory_pinn_parameters.json (deflated 48%)\nupdating: runs/test_inference/trajectory_true_parameters.json (deflated 48%)\nupdating: runs/test_inference/metrics.json (deflated 48%)\nupdating: runs/test_inference/comparison_plot.png (deflated 9%)\n  adding: runs/run_20260112_152535/ (stored 0%)\n  adding: runs/run_20260112_152535/metrics.csv (deflated 51%)\n  adding: runs/run_20260112_152535/test_results.txt (deflated 66%)\n  adding: runs/run_20260112_152535/checkpoints/ (stored 0%)\n  adding: runs/run_20260112_152535/checkpoints/best_model.pth (deflated 11%)\n  adding: runs/run_20260112_152535/events.out.tfevents.1768231537.2429de81f59c.221803.0 (deflated 67%)\n","output_type":"stream"}],"execution_count":72},{"id":"f24a4918-2ee8-4557-be9e-f1e77c397c1a","cell_type":"code","source":"!rm -rf runs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T15:23:49.823564Z","iopub.execute_input":"2026-01-12T15:23:49.824230Z","iopub.status.idle":"2026-01-12T15:23:49.940499Z","shell.execute_reply.started":"2026-01-12T15:23:49.824195Z","shell.execute_reply":"2026-01-12T15:23:49.939505Z"}},"outputs":[],"execution_count":65}]}