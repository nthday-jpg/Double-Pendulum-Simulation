{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14426279,"sourceType":"datasetVersion","datasetId":9214451}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"7324d51e","cell_type":"code","source":"import sys\nimport os\n\n# Define the absolute working directory\nbase_path = '/kaggle/working'\nproject_name = 'Double-Pendulum-Simulation'\nproject_root = os.path.join(base_path, project_name)\n\n# 1. Clone or Pull\nif not os.path.exists(project_root):\n    os.chdir(base_path)\n    !git clone https://github.com/nthday-jpg/Double-Pendulum-Simulation.git\n    print(\"Repository cloned successfully!\")\nelse:\n    os.chdir(project_root)\n    !git pull\n    print(\"Repository updated successfully!\")\n\n# 2. Correctly set the Python Path\n# It is vital to add the absolute path to the project root\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\nos.chdir(base_path)\n\n# 4. Imports\ntry:\n    import torch\n    from models.pinn import PINN\n    from training.trainer import Trainer\n    from data.dataset import get_dataloader\n    from utils.config import Config\n    print(\"All modules imported successfully!\")\nexcept ImportError as e:\n    print(f\"Import failed: {e}\")\n    print(f\"Current sys.path: {sys.path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:33:47.644661Z","iopub.execute_input":"2026-01-08T14:33:47.645038Z","iopub.status.idle":"2026-01-08T14:33:48.062486Z","shell.execute_reply.started":"2026-01-08T14:33:47.645003Z","shell.execute_reply":"2026-01-08T14:33:48.061735Z"}},"outputs":[{"name":"stdout","text":"remote: Enumerating objects: 9, done.\u001b[K\nremote: Counting objects: 100% (9/9), done.\u001b[K\nremote: Compressing objects: 100% (2/2), done.\u001b[K\nremote: Total 5 (delta 3), reused 5 (delta 3), pack-reused 0 (from 0)\u001b[K\nUnpacking objects: 100% (5/5), 839 bytes | 839.00 KiB/s, done.\nFrom https://github.com/nthday-jpg/Double-Pendulum-Simulation\n   4aef397..110e6a4  master     -> origin/master\nUpdating 4aef397..110e6a4\nFast-forward\n training/losses.py  |  9 \u001b[32m+++++++\u001b[m\u001b[31m--\u001b[m\n training/trainer.py | 22 \u001b[32m+++++++++++\u001b[m\u001b[31m-----------\u001b[m\n 2 files changed, 18 insertions(+), 13 deletions(-)\nRepository updated successfully!\nAll modules imported successfully!\n","output_type":"stream"}],"execution_count":22},{"id":"0bd8de06","cell_type":"code","source":"cfg = Config(\n    hidden_dims=[64, 64],  \n    input_dim=1,\n    output_dim=2,\n    residual_type=\"lagrangian\",\n    t_max=5, t_min=0,\n    epochs=1000,\n    early_stopping_patience = 50\n)\nmodel = PINN(cfg)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:33:48.064622Z","iopub.execute_input":"2026-01-08T14:33:48.065126Z","iopub.status.idle":"2026-01-08T14:33:48.071406Z","shell.execute_reply.started":"2026-01-08T14:33:48.065094Z","shell.execute_reply":"2026-01-08T14:33:48.070611Z"}},"outputs":[],"execution_count":23},{"id":"2c91b785","cell_type":"code","source":"input_path = \"/kaggle/input/double-pendulum\"\n\noptimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\ntrain_loader, val_loader = get_dataloader(\n    data_path=f\"{input_path}/trajectory_000.npz\",\n    parameters_path=f\"{input_path}/parameters_000.json\",\n    config=cfg\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:33:48.072590Z","iopub.execute_input":"2026-01-08T14:33:48.072860Z","iopub.status.idle":"2026-01-08T14:33:48.097405Z","shell.execute_reply.started":"2026-01-08T14:33:48.072835Z","shell.execute_reply":"2026-01-08T14:33:48.096678Z"}},"outputs":[{"name":"stdout","text":"DataLoader configured with num_workers=4, pin_memory=True\n","output_type":"stream"}],"execution_count":24},{"id":"8c837010","cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    config=cfg,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    optimizer=optimizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:33:48.098831Z","iopub.execute_input":"2026-01-08T14:33:48.099091Z","iopub.status.idle":"2026-01-08T14:33:48.109493Z","shell.execute_reply.started":"2026-01-08T14:33:48.099066Z","shell.execute_reply":"2026-01-08T14:33:48.108870Z"}},"outputs":[],"execution_count":25},{"id":"47a2a0c7","cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:33:48.110419Z","iopub.execute_input":"2026-01-08T14:33:48.111230Z","iopub.status.idle":"2026-01-08T14:33:51.168753Z","shell.execute_reply.started":"2026-01-08T14:33:48.111204Z","shell.execute_reply":"2026-01-08T14:33:51.167643Z"}},"outputs":[{"name":"stdout","text":"============================================================\nStarting training for 1000 epochs...\nDevice: cuda\nUsing Accelerator with mixed precision: no\n============================================================\nTraining complete. Logs saved to runs/run_20260108_143348\nLoss plot saved to: runs/run_20260108_143348/loss_plot.png\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/kaggle/working/Double-Pendulum-Simulation/training/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mavg_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/Double-Pendulum-Simulation/training/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, val_loader)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# Enable gradients AND use autocast to match training environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     loss, _ = compute_loss(\n","\u001b[0;32m/kaggle/working/Double-Pendulum-Simulation/training/losses.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(model, batch, weight_data, weight_phys)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Crucial: set requires_grad on a leaf tensor before the forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mq_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (N, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/Double-Pendulum-Simulation/physics/physics_loss.py\u001b[0m in \u001b[0;36mcompute_derivatives\u001b[0;34m(q, t)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mqdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         qdot[:, i] = torch.autograd.grad(\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    501\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"],"ename":"RuntimeError","evalue":"element 0 of tensors does not require grad and does not have a grad_fn","output_type":"error"}],"execution_count":26}]}