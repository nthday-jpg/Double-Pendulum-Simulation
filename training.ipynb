{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14444996,"datasetId":9214451,"databundleVersionId":15264451}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"fd213d73","cell_type":"code","source":"import os\n\n# Define paths\nbase_path = '/kaggle/working'\nproject_name = 'Double-Pendulum-Simulation'\nproject_root = os.path.join(base_path, project_name)\n\n# Clone or pull repository\nif not os.path.exists(project_root):\n    os.chdir(base_path)\n    !git clone https://github.com/nthday-jpg/Double-Pendulum-Simulation.git\n    print(\"Repository cloned successfully!\")\nelse:\n    os.chdir(project_root)\n    !git pull\n    print(\"Repository updated successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:24:27.610263Z","iopub.execute_input":"2026-01-09T16:24:27.610596Z","iopub.status.idle":"2026-01-09T16:24:28.031674Z","shell.execute_reply.started":"2026-01-09T16:24:27.610568Z","shell.execute_reply":"2026-01-09T16:24:28.030982Z"}},"outputs":[{"name":"stdout","text":"remote: Enumerating objects: 11, done.\u001b[K\nremote: Counting objects: 100% (11/11), done.\u001b[K\nremote: Compressing objects: 100% (2/2), done.\u001b[K\nremote: Total 6 (delta 4), reused 6 (delta 4), pack-reused 0 (from 0)\u001b[K\nUnpacking objects: 100% (6/6), 643 bytes | 321.00 KiB/s, done.\nFrom https://github.com/nthday-jpg/Double-Pendulum-Simulation\n   3dbb23b..a21148a  master     -> origin/master\nUpdating 3dbb23b..a21148a\nFast-forward\n data/dataset.py | 8 \u001b[32m++++\u001b[m\u001b[31m----\u001b[m\n utils/config.py | 1 \u001b[32m+\u001b[m\n 2 files changed, 5 insertions(+), 4 deletions(-)\nRepository updated successfully!\n","output_type":"stream"}],"execution_count":5},{"id":"d18e4d95","cell_type":"code","source":"# Generate training data\n# Generate 10 trajectories with 5000 points each\n!python {project_root}/generate_data.py \\\n    --output_dir {project_root}/data/raw \\\n    --num_trajectories 1 \\\n    --num_points 3000 \\\n    --t_start 0.0 \\\n    --t_end 5.0 \\\n    --check_energy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:25:12.712034Z","iopub.execute_input":"2026-01-09T16:25:12.712810Z","iopub.status.idle":"2026-01-09T16:25:14.716661Z","shell.execute_reply.started":"2026-01-09T16:25:12.712767Z","shell.execute_reply":"2026-01-09T16:25:14.715976Z"}},"outputs":[{"name":"stdout","text":"Generating 1 trajectories...\nDeriving equations symbolically (this may take a moment)...\nSymbolic derivation complete!\n  Trajectory 000 (m1=1.00, m2=1.00, l1=1.00, l2=1.00): Energy drift = 0.000%\n  Saved: /kaggle/working/Double-Pendulum-Simulation/data/raw/trajectory_000.npz and /kaggle/working/Double-Pendulum-Simulation/data/raw/parameters_000.json\n\nDataset complete! Saved to /kaggle/working/Double-Pendulum-Simulation/data/raw\n","output_type":"stream"}],"execution_count":8},{"id":"903d90b0-9267-4782-aee2-b95e1d297b9b","cell_type":"code","source":"!rm -rf /kaggle/working/Double-Pendulum-Simulation/data/raw/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:24:55.840505Z","iopub.execute_input":"2026-01-09T16:24:55.841176Z","iopub.status.idle":"2026-01-09T16:24:55.959154Z","shell.execute_reply.started":"2026-01-09T16:24:55.841142Z","shell.execute_reply":"2026-01-09T16:24:55.958164Z"}},"outputs":[],"execution_count":7},{"id":"48eb7908","cell_type":"code","source":"# Training Configuration\n# Experiment\nseed = 42\nrun_name = None  # Auto-generated if None\ncheckpoint_path = None  # Example: \"runs/run_20260108_170349/checkpoints/best_model.pth\"\n\n# Data\nval_split = 0.2\ntest_split = 0.1\nnormalize_time = False\nnormalize_state = False\n\n# Model Architecture\nmodel = \"pinn\"  # mlp | neural_ode | hnn | pinn\nhidden_dims = \"64 64\"  # Space-separated\nactivation = \"tanh\"  # tanh | relu | gelu | silu | softplus\nuse_batch_norm = False\ndropout_rate = 0.0\nfinal_activation = None  # None | tanh | sigmoid\ninput_dim = 1\noutput_dim = 2\n\n# Training\nlr = 0.001\nbatch_size = 128\nbatch_size_collocation = 1024\nepochs = 1000\noptimizer = \"adam\"  # adam | adamw | sgd\nweight_decay = 0.0\ngrad_clip = None\nscheduler = None  # None | cosine | step\n\n# PyTorch Optimizations\nuse_compile = False\ncompile_mode = \"default\"  # default | reduce-overhead | max-autotune\nmixed_precision = False\ngradient_accumulation_steps = 1\n\n# Regularization\nl1_lambda = 0.0\nl2_lambda = 0.0\n\n# Physics / PINN\nuse_physics = True\nn_collocation = 5000\ndata_fraction = 0.1\nphysics_weight = 1.0\ndata_weight = 5.0\nresidual_type = \"lagrangian\"  # eom | lagrangian | hamiltonian\n\n# Time Domain\nt_min = 0.0\nt_max = 5.0\ncollocation_sampling = \"uniform\"  # uniform | random | latin_hypercube\n\n# Rollout Evaluation\nrollout_T = 5.0\nrollout_dt = 0.01\n\n# Logging\nlog_interval = 1\nprint_interval = 10\nsave_checkpoints = True\ncheckpoint_interval = 50\n\n# Early Stopping\nearly_stopping_patience = 50  # None to disable\n\n# Physical Parameters\nm1 = 1.0\nm2 = 1.0\nl1 = 1.0\nl2 = 1.0\ng = 9.81\n\n# Build command arguments\nargs_list = [\n    f\"--seed {seed}\",\n    f\"--hidden_dims {hidden_dims}\",\n    f\"--input_dim {input_dim}\",\n    f\"--output_dim {output_dim}\",\n    f\"--residual_type {residual_type}\",\n    f\"--t_max {t_max}\",\n    f\"--t_min {t_min}\",\n    f\"--epochs {epochs}\",\n    f\"--lr {lr}\",\n    f\"--batch_size {batch_size}\",\n    f\"--batch_size_collocation {batch_size_collocation}\",\n    f\"--val_split {val_split}\",\n    f\"--test_split {test_split}\",\n    f\"--model {model}\",\n    f\"--activation {activation}\",\n    f\"--dropout_rate {dropout_rate}\",\n    f\"--optimizer {optimizer}\",\n    f\"--weight_decay {weight_decay}\",\n    f\"--compile_mode {compile_mode}\",\n    f\"--gradient_accumulation_steps {gradient_accumulation_steps}\",\n    f\"--l1_lambda {l1_lambda}\",\n    f\"--l2_lambda {l2_lambda}\",\n    f\"--n_collocation {n_collocation}\",\n    f\"--data_fraction {data_fraction}\",\n    f\"--physics_weight {physics_weight}\",\n    f\"--data_weight {data_weight}\",\n    f\"--collocation_sampling {collocation_sampling}\",\n    f\"--rollout_T {rollout_T}\",\n    f\"--rollout_dt {rollout_dt}\",\n    f\"--log_interval {log_interval}\",\n    f\"--print_interval {print_interval}\",\n    f\"--checkpoint_interval {checkpoint_interval}\",\n    f\"--m1 {m1}\",\n    f\"--m2 {m2}\",\n    f\"--l1 {l1}\",\n    f\"--l2 {l2}\",\n    f\"--g {g}\"\n]\n\n# Add optional flags\nif run_name:\n    args_list.append(f\"--run_name {run_name}\")\nif checkpoint_path:\n    args_list.append(f\"--checkpoint_path {checkpoint_path}\")\nif use_compile:\n    args_list.append(\"--use_compile\")\nif normalize_time:\n    args_list.append(\"--normalize_time\")\nif normalize_state:\n    args_list.append(\"--normalize_state\")\nif use_batch_norm:\n    args_list.append(\"--use_batch_norm\")\nif final_activation:\n    args_list.append(f\"--final_activation {final_activation}\")\nif grad_clip:\n    args_list.append(f\"--grad_clip {grad_clip}\")\nif scheduler:\n    args_list.append(f\"--scheduler {scheduler}\")\nif mixed_precision:\n    args_list.append(\"--mixed_precision\")\nif save_checkpoints:\n    args_list.append(\"--save_checkpoints\")\nif early_stopping_patience:\n    args_list.append(f\"--early_stopping_patience {early_stopping_patience}\")\nif use_physics:\n    args_list.append(\"--use_physics\")\n\nargs = \" \".join(args_list)\n\nprint(f\"Training Configuration:\")\nprint(f\"  Seed: {seed}\")\nprint(f\"  Model: {model}, Hidden: {hidden_dims}, Activation: {activation}\")\nprint(f\"  Optimizer: {optimizer}, LR: {lr}, Weight Decay: {weight_decay}\")\nprint(f\"  Epochs: {epochs}, Early Stop: {early_stopping_patience if early_stopping_patience else 'disabled'}\")\nprint(f\"  Batch sizes: data={batch_size}, colloc={batch_size_collocation}\")\nprint(f\"  Physics: weight={physics_weight}, data_weight={data_weight}, type={residual_type}\")\nprint(f\"  Checkpoint: {checkpoint_path if checkpoint_path else 'None (training from scratch)'}\")\nprint(f\"\\nCommand arguments generated with {len(args_list)} parameters\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:25:34.002086Z","iopub.execute_input":"2026-01-09T16:25:34.002890Z","iopub.status.idle":"2026-01-09T16:25:34.015926Z","shell.execute_reply.started":"2026-01-09T16:25:34.002858Z","shell.execute_reply":"2026-01-09T16:25:34.015119Z"}},"outputs":[{"name":"stdout","text":"Training Configuration:\n  Seed: 42\n  Model: pinn, Hidden: 64 64, Activation: tanh\n  Optimizer: adam, LR: 0.001, Weight Decay: 0.0\n  Epochs: 1000, Early Stop: 50\n  Batch sizes: data=128, colloc=1024\n  Physics: weight=1.0, data_weight=5.0, type=lagrangian\n  Checkpoint: None (training from scratch)\n\nCommand arguments generated with 40 parameters\n","output_type":"stream"}],"execution_count":9},{"id":"1c12c226","cell_type":"code","source":"# Launch distributed training with Accelerate\n!accelerate launch --num_processes=2 {project_root}/train.py {args}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:25:40.645115Z","iopub.execute_input":"2026-01-09T16:25:40.645608Z","iopub.status.idle":"2026-01-09T16:26:23.776546Z","shell.execute_reply.started":"2026-01-09T16:25:40.645580Z","shell.execute_reply":"2026-01-09T16:26:23.775819Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\nThe following values were not passed to `accelerate launch` and had defaults used instead:\n\t\tMore than one GPU was found, enabling multi-GPU training.\n\t\tIf this was unintended please pass in `--num_processes=1`.\n\t`--num_machines` was set to a value of `1`\n\t`--mixed_precision` was set to a value of `'no'`\n\t`--dynamo_backend` was set to a value of `'no'`\nTo avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767975965.676396     233 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767975965.676385     234 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767975965.760085     233 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1767975965.760098     234 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767975966.385387     234 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767975966.385391     233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767975966.385452     234 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767975966.385452     233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767975966.385458     234 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767975966.385461     233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767975966.385464     234 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767975966.385468     233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nusage: train.py [-h] [--run_name RUN_NAME] [--seed SEED] [--data_dir DATA_DIR]\n                [--val_split VAL_SPLIT] [--normalize_time] [--normalize_state]\n                [--model {mlp,neural_ode,hnn,pinn}]\n                [--hidden_dims HIDDEN_DIMS [HIDDEN_DIMS ...]]\n                [--activation {tanh,relu,gelu,silu,softplus}]\n                [--use_batch_norm] [--dropout_rate DROPOUT_RATE]\n                [--final_activation {tanh,sigmoid}] [--input_dim INPUT_DIM]\n                [--output_dim OUTPUT_DIM] [--lr LR] [--batch_size BATCH_SIZE]\n                [--batch_size_collocation BATCH_SIZE_COLLOCATION]\n                [--epochs EPOCHS] [--optimizer {adam,adamw,sgd}]\n                [--weight_decay WEIGHT_DECAY] [--grad_clip GRAD_CLIP]\n                [--scheduler {cosine,step}] [--use_compile]\n                [--compile_mode {default,reduce-overhead,max-autotune}]\n                [--mixed_precision]\n                [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n                [--checkpoint_path CHECKPOINT_PATH] [--l1_lambda L1_LAMBDA]\n                [--l2_lambda L2_LAMBDA] [--use_physics]\n                [--n_collocation N_COLLOCATION]\n                [--data_fraction DATA_FRACTION]\n                [--physics_weight PHYSICS_WEIGHT] [--data_weight DATA_WEIGHT]\n                [--residual_type {eom,lagrangian,hamiltonian}] [--t_min T_MIN]\n                [--t_max T_MAX]\n                [--collocation_sampling {uniform,random,latin_hypercube}]\n                [--rollout_T ROLLOUT_T] [--rollout_dt ROLLOUT_DT]\n                [--log_interval LOG_INTERVAL]\n                [--print_interval PRINT_INTERVAL] [--save_checkpoints]\n                [--checkpoint_interval CHECKPOINT_INTERVAL]\n                [--early_stopping_patience EARLY_STOPPING_PATIENCE] [--m1 M1]\n                [--m2 M2] [--l1 L1] [--l2 L2] [--g G]\ntrain.py: error: unrecognized arguments: --test_split 0.1\nusage: train.py [-h] [--run_name RUN_NAME] [--seed SEED] [--data_dir DATA_DIR]\n                [--val_split VAL_SPLIT] [--normalize_time] [--normalize_state]\n                [--model {mlp,neural_ode,hnn,pinn}]\n                [--hidden_dims HIDDEN_DIMS [HIDDEN_DIMS ...]]\n                [--activation {tanh,relu,gelu,silu,softplus}]\n                [--use_batch_norm] [--dropout_rate DROPOUT_RATE]\n                [--final_activation {tanh,sigmoid}] [--input_dim INPUT_DIM]\n                [--output_dim OUTPUT_DIM] [--lr LR] [--batch_size BATCH_SIZE]\n                [--batch_size_collocation BATCH_SIZE_COLLOCATION]\n                [--epochs EPOCHS] [--optimizer {adam,adamw,sgd}]\n                [--weight_decay WEIGHT_DECAY] [--grad_clip GRAD_CLIP]\n                [--scheduler {cosine,step}] [--use_compile]\n                [--compile_mode {default,reduce-overhead,max-autotune}]\n                [--mixed_precision]\n                [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n                [--checkpoint_path CHECKPOINT_PATH] [--l1_lambda L1_LAMBDA]\n                [--l2_lambda L2_LAMBDA] [--use_physics]\n                [--n_collocation N_COLLOCATION]\n                [--data_fraction DATA_FRACTION]\n                [--physics_weight PHYSICS_WEIGHT] [--data_weight DATA_WEIGHT]\n                [--residual_type {eom,lagrangian,hamiltonian}] [--t_min T_MIN]\n                [--t_max T_MAX]\n                [--collocation_sampling {uniform,random,latin_hypercube}]\n                [--rollout_T ROLLOUT_T] [--rollout_dt ROLLOUT_DT]\n                [--log_interval LOG_INTERVAL]\n                [--print_interval PRINT_INTERVAL] [--save_checkpoints]\n                [--checkpoint_interval CHECKPOINT_INTERVAL]\n                [--early_stopping_patience EARLY_STOPPING_PATIENCE] [--m1 M1]\n                [--m2 M2] [--l1 L1] [--l2 L2] [--g G]\ntrain.py: error: unrecognized arguments: --test_split 0.1\nE0109 16:26:22.386000 214 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 2) local_rank: 0 (pid: 233) of binary: /usr/bin/python3\nTraceback (most recent call last):\n  File \"/usr/local/bin/accelerate\", line 10, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n    args.func(args)\n  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 1226, in launch_command\n    multi_gpu_launcher(args)\n  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 853, in multi_gpu_launcher\n    distrib_run.run(args)\n  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 892, in run\n    elastic_launch(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 277, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\n/kaggle/working/Double-Pendulum-Simulation/train.py FAILED\n------------------------------------------------------------\nFailures:\n[1]:\n  time      : 2026-01-09_16:26:22\n  host      : c7c729484306\n  rank      : 1 (local_rank: 1)\n  exitcode  : 2 (pid: 234)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2026-01-09_16:26:22\n  host      : c7c729484306\n  rank      : 0 (local_rank: 0)\n  exitcode  : 2 (pid: 233)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n","output_type":"stream"}],"execution_count":10}]}